# HOW TO RUN: Apply Explainability using SHAP or LIME and Audit Model Fairness

--------------------------------------
ğŸ”¹ SETUP
--------------------------------------
1ï¸âƒ£ Install dependencies:
   pip install -r requirements.txt

--------------------------------------
ğŸ”¹ OPTION 1: Custom Dataset (Custom)
--------------------------------------
1ï¸âƒ£ Ensure the CSV file path and column names in explainability_fairness_generic.py are correctly set:
CSV_PATH = "sample.csv"        # path to your dataset  
TARGET = "gender"              # target column  
SENSITIVE_FEATURE = "gender"   # sensitive attribute  
2ï¸âƒ£ Run the script:
python explainability_fairness_generic.py
3ï¸âƒ£ Outputs (saved in your current folder):
âœ… model_output_shap_summary.png â€” SHAP global feature importance
âœ… model_output_shap_waterfall.png â€” SHAP local explanation (sample 0)
âœ… model_output_lime_explanation.html â€” LIME local explanation (HTML)
âœ… model_output_lime_plot.png â€” LIME visualization plot
âœ… model_output_fairness.png â€” Fairness metrics by sensitive feature

--------------------------------------
ğŸ”¹ OPTION 2: Toy Iris Dataset (no file needed)
--------------------------------------
1ï¸âƒ£ Run:
   python explainability_fairness_toy.py
2ï¸âƒ£ Outputs:
   âœ… shap_summary_iris.png  
   âœ… lime_explanation_iris.html  
   âœ… fairness_audit_iris.png  

Key Note:

â€œWhen SHAP plots appear, you must close the window manually â€” it then continues with LIME and Fairlearn.â€

--------------------------------------
ğŸ”¹ WHAT YOUâ€™LL SEE
--------------------------------------
- **SHAP** â†’ Feature importance and contribution analysis
- **LIME** â†’ Local explanation for a single prediction
- **Fairlearn** â†’ Bias analysis between "male" and "female" synthetic groups

--------------------------------------
âœ… DONE!
--------------------------------------
Youâ€™ve successfully implemented SHAP and LIME explainability
and fairness auditing for both a real and toy dataset.
